{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3803280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pradhyumnaa G\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3daf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13784, 2)\n",
      "Test : (700, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load CSVs\n",
    "\n",
    "train_df = pd.read_csv(\"dataset/snips_intent_train.csv\")\n",
    "test_df  = pd.read_csv(\"dataset/snips_intent_test.csv\")\n",
    "\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Test :\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef12dd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Add another song to the Cita Romántica playlist.</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add clem burke in my playlist Pre-Party R&amp;B Jams</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Add Live from Aragon Ballroom to Trapeo</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>add Unite and Win to my night out</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add track to my Digster Future Hits</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text         intent\n",
       "0  Add another song to the Cita Romántica playlist.  AddToPlaylist\n",
       "1  add clem burke in my playlist Pre-Party R&B Jams  AddToPlaylist\n",
       "2           Add Live from Aragon Ballroom to Trapeo  AddToPlaylist\n",
       "3                 add Unite and Win to my night out  AddToPlaylist\n",
       "4               Add track to my Digster Future Hits  AddToPlaylist"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize whitespace and ensure all text fields are clean strings\n",
    "\n",
    "def clean_text(s):\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s))\n",
    "    return s.strip()\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text']  = test_df['text'].apply(clean_text)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e789e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode intent labels as integers and save the label ↔ id mappings\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_df['label'] = le.fit_transform(train_df['intent'])\n",
    "test_df['label']  = le.transform(test_df['intent'])\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# Save label mappings\n",
    "label2id = {label: int(i) for i, label in enumerate(le.classes_)}\n",
    "id2label = {int(i): label for i, label in enumerate(le.classes_)}\n",
    "\n",
    "with open(\"label2id.json\", \"w\") as f:\n",
    "    json.dump(label2id, f, indent=2)\n",
    "\n",
    "with open(\"id2label.json\", \"w\") as f:\n",
    "    json.dump(id2label, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4fdb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "#MODEL_NAME = \"distilbert-base-uncased\" #<- Use This for Distilbert\n",
    "MODEL_NAME = \"google-bert/bert-base-uncased\" #<- Use This for BERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff9222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes a list of texts into padded, truncated transformer inputs\n",
    "\n",
    "def tokenize(df):\n",
    "    return tokenizer(\n",
    "        df[\"text\"].tolist(),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebaa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch dataset wrapper for encoded SNIPS intent samples\n",
    "\n",
    "class SnipsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.encodings = tokenize(df)\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3746c6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in Train: 289\n",
      "Duplicates in Test: 6\n",
      "New splits:\n",
      "Train: (12248, 3)\n",
      "Val  : (1361, 3)\n",
      "Test : (697, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intent\n",
       "GetWeather              199\n",
       "PlayMusic               198\n",
       "BookRestaurant          197\n",
       "SearchCreativeWork      195\n",
       "AddToPlaylist           194\n",
       "RateBook                192\n",
       "SearchScreeningEvent    186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training set into TRAIN + VAL\n",
    "\n",
    "duplicates1 = train_df[train_df.duplicated(subset=[\"text\", \"intent\"], keep=False)]\n",
    "print(f\"Duplicates in Train: {len(duplicates1)}\")\n",
    "\n",
    "duplicates2 = test_df[test_df.duplicated(subset=[\"text\", \"intent\"], keep=False)]\n",
    "print(f\"Duplicates in Test: {len(duplicates2)}\")\n",
    "\n",
    "# Remove duplicates within.\n",
    "train_df = train_df.drop_duplicates(subset=[\"text\", \"intent\"]).reset_index(drop=True)\n",
    "test_df  = test_df.drop_duplicates(subset=[\"text\", \"intent\"]).reset_index(drop=True)\n",
    "\n",
    "# Remove Duplicates Across Train and Test\n",
    "test_pairs = set(zip(test_df[\"text\"], test_df[\"intent\"]))\n",
    "\n",
    "train_df = train_df[\n",
    "    ~train_df.apply(lambda row: (row[\"text\"], row[\"intent\"]) in test_pairs, axis=1)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Then split training into train + val\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    stratify=train_df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"New splits:\")\n",
    "print(\"Train:\", train_df.shape)\n",
    "print(\"Val  :\", val_df.shape)\n",
    "print(\"Test :\", test_df.shape)\n",
    "\n",
    "val_df[\"intent\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c7558cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overlap between TRAIN and VAL: 0 ===\n",
      "\n",
      "=== Overlap between TRAIN and TEST: 0 ===\n",
      "\n",
      "=== Overlap between VAL and TEST: 0 ===\n"
     ]
    }
   ],
   "source": [
    "# Helper function to list duplicates between two datasets\n",
    "\n",
    "def find_overlap(df1, df2, name1=\"DF1\", name2=\"DF2\"):\n",
    "    pairs1 = set(zip(df1[\"text\"], df1[\"intent\"]))\n",
    "    pairs2 = set(zip(df2[\"text\"], df2[\"intent\"]))\n",
    "    overlap = pairs1 & pairs2\n",
    "\n",
    "    print(f\"\\n=== Overlap between {name1} and {name2}: {len(overlap)} ===\")\n",
    "    for text, intent in list(overlap):\n",
    "        print(f\"[{intent}] {text}\")\n",
    "    if len(overlap) > 25:\n",
    "        print(\"... (truncated)\")\n",
    "    return overlap\n",
    "\n",
    "# Check all combinations\n",
    "\n",
    "overlap_train_val  = find_overlap(train_df, val_df,  \"TRAIN\", \"VAL\")\n",
    "overlap_train_test = find_overlap(train_df, test_df, \"TRAIN\", \"TEST\")\n",
    "overlap_val_test   = find_overlap(val_df,  test_df, \"VAL\",   \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0307431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12248, 697, 1361)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build datasets\n",
    "\n",
    "train_dataset = SnipsDataset(train_df)\n",
    "val_dataset   = SnipsDataset(val_df)\n",
    "test_dataset  = SnipsDataset(test_df)\n",
    "\n",
    "len(train_dataset), len(test_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf86747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building 10-shot dataset ===\n",
      "✓ Saved fewshot_datasets/snips_train_dataset_10.pt  |  size=70\n",
      "\n",
      "=== Building 20-shot dataset ===\n",
      "✓ Saved fewshot_datasets/snips_train_dataset_20.pt  |  size=140\n",
      "\n",
      "=== Building 50-shot dataset ===\n",
      "✓ Saved fewshot_datasets/snips_train_dataset_50.pt  |  size=350\n",
      "\n",
      "=== Building 70-shot dataset ===\n",
      "✓ Saved fewshot_datasets/snips_train_dataset_70.pt  |  size=490\n",
      "\n",
      "=== Building 100-shot dataset ===\n",
      "✓ Saved fewshot_datasets/snips_train_dataset_100.pt  |  size=700\n",
      "\n",
      "Few-shot dataset generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pradhyumnaa G\\AppData\\Local\\Temp\\ipykernel_26508\\2019982166.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n",
      "C:\\Users\\Pradhyumnaa G\\AppData\\Local\\Temp\\ipykernel_26508\\2019982166.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n",
      "C:\\Users\\Pradhyumnaa G\\AppData\\Local\\Temp\\ipykernel_26508\\2019982166.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n",
      "C:\\Users\\Pradhyumnaa G\\AppData\\Local\\Temp\\ipykernel_26508\\2019982166.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n",
      "C:\\Users\\Pradhyumnaa G\\AppData\\Local\\Temp\\ipykernel_26508\\2019982166.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# 9 Few Shot Generation (10, 20, 50, 70, 100 per intent)\n",
    "\n",
    "SHOT_SIZES = [10, 20, 50, 70, 100]\n",
    "\n",
    "def sample_few_shot(df, n_per_intent):\n",
    "    return (\n",
    "        df.groupby(\"intent\", group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=min(n_per_intent, len(x)), random_state=42))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def build_and_save_pt(df, filename):\n",
    "    dataset = SnipsDataset(df)\n",
    "    torch.save(dataset, filename)\n",
    "    print(f\"✓ Saved {filename}  |  size={len(df)}\")\n",
    "\n",
    "import os\n",
    "os.makedirs(\"fewshot_datasets\", exist_ok=True)\n",
    "\n",
    "for shot in SHOT_SIZES:\n",
    "    print(f\"\\n=== Building {shot}-shot dataset ===\")\n",
    "\n",
    "    # sample from train split only\n",
    "    df_small = sample_few_shot(train_df, shot)\n",
    "\n",
    "    # Save CSV for inspection\n",
    "    csv_path = f\"fewshot_datasets/snips_train_{shot}.csv\"\n",
    "    df_small.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save .pt file after tokenization\n",
    "    pt_path = f\"fewshot_datasets/snips_train_dataset_{shot}.pt\"\n",
    "    build_and_save_pt(df_small, pt_path)\n",
    "\n",
    "print(\"\\nFew-shot dataset generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20f2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train / val / test .pt files\n"
     ]
    }
   ],
   "source": [
    "# Save tokenized datasets\n",
    "\n",
    "torch.save(train_dataset, \"dataset/snips_train_dataset.pt\")\n",
    "torch.save(val_dataset,   \"dataset/snips_val_dataset.pt\")\n",
    "torch.save(test_dataset,  \"dataset/snips_test_dataset.pt\")\n",
    "\n",
    "print(\"Saved: train / val / test .pt files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33cacdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.serialization import add_safe_globals\n",
    "from __main__ import SnipsDataset\n",
    "\n",
    "add_safe_globals([SnipsDataset])\n",
    "\n",
    "dataset = torch.load(\"dataset/snips_train_dataset.pt\", weights_only=False)\n",
    "\n",
    "with open(\"id2label.json\", \"r\") as f:\n",
    "    id2label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1592e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2054,  6579,  5691,  2024,  1999,  1996, 10971,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(6)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = random.randint(0,len(train_df))\n",
    "sample = dataset[seed]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ce01aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what animated movies are in the neighbourhood\n",
      "Label: SearchScreeningEvent (6)\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(text)\n",
    "label_id = int(sample['labels'])\n",
    "label_text = id2label[str(label_id)]\n",
    "print(\"Label:\", label_text, f\"({label_id})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
