{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f8cee0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Building Practical Intent Classification and NER Pipelines on the SNIPS Dataset**\n",
    "\n",
    "This project implements practical, well-structured NLP pipelines on the **SNIPS 2018 Spoken Language Understanding dataset**, focusing on two tasks:\n",
    "\n",
    "1. **Intent Classification**\n",
    "2. **Slot Filling (NER)**\n",
    "\n",
    "The notebook is structured — and this README follows the same structure — by covering **Intent Classification first**, then **Slot Filling** as a separate second part.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a53088",
   "metadata": {},
   "source": [
    "# **Dataset Citation**\n",
    "\n",
    "> Coucke A. et al., “Snips Voice Platform: an embedded spoken language understanding system for private-by-design voice interfaces.” 2018.\n",
    "> [https://arxiv.org/abs/1805.10190](https://arxiv.org/abs/1805.10190)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163e943",
   "metadata": {},
   "source": [
    "# **Dataset Overview**\n",
    "\n",
    "The SNIPS dataset contains user utterances annotated for two SLU tasks:\n",
    "\n",
    "### **1. Intent Classification**\n",
    "\n",
    "* 7 intent labels\n",
    "* Well-Balanced\n",
    "* Each utterance has exactly one intent\n",
    "  (e.g., `BookRestaurant`, `SearchCreativeWork`, `PlayMusic`, etc.)\n",
    "\n",
    "### **2. Slot Filling (NER)**\n",
    "\n",
    "Utterances contain entity spans such as:\n",
    "\n",
    "* `object_type`\n",
    "* `artist`\n",
    "* `genre`\n",
    "* `country`\n",
    "* `poi_type`\n",
    "* etc.\n",
    "\n",
    "Entity frequencies are **very imbalanced** (e.g., `object_type`: 3341 vs `genre`: 147), which heavily influences NER difficulty.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd30b75",
   "metadata": {},
   "source": [
    "# **PART I — Intent Classification**\n",
    "\n",
    "## **1. Summary**\n",
    "\n",
    "This section covers the **intent classification pipeline** using **BERT-base**, evaluated under:\n",
    "\n",
    "* **Few-shot settings:** 10, 20, 50, 70, 100 samples per intent\n",
    "* **Full-data setting**\n",
    "\n",
    "All experiments use the **cleaned, leakage-free** dataset.\n",
    "\n",
    "**Top-level results :**\n",
    "\n",
    "* **98.56% accuracy** on the full dataset\n",
    "* **97.84% accuracy** with only 100 samples/intent\n",
    "* Detailed shot-by-shot table below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16448a7",
   "metadata": {},
   "source": [
    "## **2. Data Cleaning & Preprocessing**\n",
    "\n",
    "The intent preprocessing pipeline includes:\n",
    "\n",
    "* Whitespace normalization\n",
    "* Stratified train–validation split (preserves label distribution)\n",
    "* Deduplication within each split\n",
    "* Strict removal of cross-split leakage (train ↔ val ↔ test)\n",
    "* Consistent text formatting across all few-shot and full-data settings\n",
    "\n",
    "This ensures clean, balanced, leakage-free evaluation across all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85779f",
   "metadata": {},
   "source": [
    "## **3. Modeling Approach**\n",
    "\n",
    "* **Model:** BERT-base (fine-tuned end-to-end)\n",
    "* **Head:** Single linear classification layer on `[CLS]`\n",
    "* **Training:** Cross-entropy loss, AdamW optimizer, linear LR scheduler with warmup, early stopping (patience = 3)\n",
    "* **Validation:** Best model selected using validation weighted F1\n",
    "* **Inputs:** Cleaned, tokenized utterances (max length = 44)\n",
    "* **Evaluation:** Accuracy, weighted F1, confusion matrix, error analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcbd3b",
   "metadata": {},
   "source": [
    "## **4. Results**\n",
    "\n",
    "| Training Size | Accuracy | Weighted F1 | Errors (out of 697) |\n",
    "| ------------- | -------- | ----------- | ------------------- |\n",
    "| **10-shot**   | 82.21%   | 81.13%      | 124                 |\n",
    "| **20-shot**   | 95.69%   | 95.66%      | 30                  |\n",
    "| **50-shot**   | 96.70%   | 96.72%      | 23                  |\n",
    "| **70-shot**   | 97.84%   | 97.86%      | 15                  |\n",
    "| **100-shot**  | 97.84%   | 97.86%      | 15                  |\n",
    "| **Full**      | 98.56%   | 98.57%      | 10                  |\n",
    "\n",
    "### **Confusion Matrix of the Full Training Size Model**\n",
    "\n",
    "![image info](output.png)\n",
    "\n",
    "### **Observations**\n",
    "\n",
    "* Strong scaling with labeled data, diminishing returns after ~70 examples per intent.\n",
    "* Remaining errors mostly come from semantically close intent pairs:\n",
    "\n",
    "  * *SearchCreativeWork* ↔ *SearchScreeningEvent*\n",
    "* A small set of utterances is genuinely ambiguous/underspecified.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe268af",
   "metadata": {},
   "source": [
    "# **PART II — Slot Filling (NER)**\n",
    "\n",
    "## **1. Summary**\n",
    "\n",
    "This section implements the **slot filling (entity recognition)** pipeline using BERT-base with BIO tagging.\n",
    "\n",
    "Core focus:\n",
    "\n",
    "* Correct subword → label alignment\n",
    "* Clean handling of tokenization drift\n",
    "* Reliable entity-level evaluation\n",
    "\n",
    "**NER Result (entity-level F1):**\n",
    "**94.54%**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39770daf",
   "metadata": {},
   "source": [
    "## **2. Data Cleaning & Preprocessing (NER)**\n",
    "\n",
    "The NER preprocessing pipeline includes:\n",
    "\n",
    "* Reconstructing full utterances from SNIPS JSON files\n",
    "* Extracting character-level entity spans from the original annotations\n",
    "* Performing whitespace-based tokenization\n",
    "* Converting entity spans into **BIO word-level tags**\n",
    "* Applying a stratified train–validation split by intent\n",
    "* Ensuring consistent formatting across train/validation/test sets\n",
    "\n",
    "This creates clean word-level BIO labels ready for subword alignment during tokenization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb3254",
   "metadata": {},
   "source": [
    "## **3. Modeling**\n",
    "\n",
    "* **Model:** DistilBERT (fine-tuned end-to-end)  \n",
    "* **Head:** Token-classification layer over subword tokens  \n",
    "* **Labeling scheme:** BIO tags aligned to subword tokens  \n",
    "* **Training:** AdamW optimizer, linear LR scheduler, warmup, early stopping  \n",
    "* **Masking:** `-100` used to ignore special and padded tokens  \n",
    "* **Evaluation:** Entity-level F1 using `seqeval`, with token-level accuracy as an auxiliary metric\n",
    "\n",
    "Training, subword alignment, and evaluation are implemented directly in the notebook using HuggingFace's `Trainer`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5696a",
   "metadata": {},
   "source": [
    "## **4. Results**\n",
    "\n",
    "* **Entity-level F1:** **94.54%**\n",
    "\n",
    "Entity-type variability is substantial due to frequency imbalance:\n",
    "\n",
    "* High-frequency entities (e.g., `object_type`) → very strong F1\n",
    "* Sparse entities (e.g., `genre`) → lower stability in few-shot conditions\n",
    "\n",
    "Detailed per-entity breakdown and analysis are available in the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199a7d4",
   "metadata": {},
   "source": [
    "# **How to Use This Repository**\n",
    "\n",
    "1. **Install dependencies**\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. **Run the Intent Classification pipeline**\n",
    "\n",
    "Execute notebooks **in order**:\n",
    "\n",
    "```\n",
    "01_build_snips_intent_dataset.ipynb  \n",
    "02_preprocessing_intent.ipynb  \n",
    "03_eda_intent.ipynb  \n",
    "04_train_and_evaluate_full_intent.ipynb  \n",
    "05_evaluate_intent.ipynb  \n",
    "06_prototype_intent.ipynb\n",
    "```\n",
    "\n",
    "This builds the intent dataset, preprocesses it, trains the model (few-shot + full), evaluates it, and produces the confusion matrix.\n",
    "\n",
    "3. **Run the NER pipeline**\n",
    "\n",
    "Execute notebooks **in order**:\n",
    "\n",
    "```\n",
    "07_build_snips_ner_dataset.ipynb  \n",
    "08_train_and_evaluate_full_ner.ipynb  \n",
    "09_evaluate_ner.ipynb  \n",
    "10_prototype_ner.ipynb   (if present)\n",
    "```\n",
    "\n",
    "This constructs the NER dataset (BIO word-level), aligns labels to subwords, trains DistilBERT, and evaluates using entity-level F1."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
